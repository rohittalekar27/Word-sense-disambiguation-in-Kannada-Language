{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tkinter as tk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    with open('Dataset.json', 'r', encoding='utf-8') as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    # Extract the necessary information from the dataset\n",
    "    data = []\n",
    "    for item in dataset:\n",
    "        word = item['word']\n",
    "        translation = item['translation']\n",
    "        senses = item['senses']\n",
    "        meanings = [item['disambiguation']]\n",
    "        data.append({'Word': word, 'Translation': translation, 'Sense': senses, 'Meaning': '; '.join(meanings)})\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Functions\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Zà²€-\\u25FF\\u2600-\\u26FF\\u2700-\\u27BF]', ' ', text)\n",
    "\n",
    "    # Tokenize the text into individual words\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction Functions\n",
    "def extract_features(word):\n",
    "    # Example feature extraction: Part-of-speech tagging\n",
    "    pos_tags = pos_tag([word])\n",
    "    features = {}\n",
    "    if len(pos_tags) > 0:\n",
    "        features['POS'] = pos_tags[0][1]\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule-Based Disambiguation\n",
    "def rule_based_disambiguate_word(word, dataset, features):\n",
    "    # Iterate through the dataset and check for word matches\n",
    "    for index, row in dataset.iterrows():\n",
    "        if row['Word'] == word or row['Translation'] == word:\n",
    "            # Check if all the features match\n",
    "            if all(feature in row for feature in features.values()):\n",
    "                return row['Meaning']\n",
    "\n",
    "    # If no match is found, return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "def train_model(dataset):\n",
    "    # Preprocess the dataset\n",
    "    dataset['Preprocessed'] = dataset['Meaning'].apply(preprocess_text)\n",
    "\n",
    "    # Extract features using TF-IDF vectorization\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(dataset['Preprocessed'])\n",
    "    y = dataset['Sense']\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the LinearSVC model\n",
    "    model = LinearSVC()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate accuracy on the testing set\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model accuracy: {accuracy}\")\n",
    "\n",
    "    return model, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Prediction\n",
    "def predict_sense(text, dataset, model, vectorizer):\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    words = preprocessed_text.split()\n",
    "    word_senses = {}\n",
    "    ambiguous_words = {}\n",
    "\n",
    "    for word in words:\n",
    "        features = extract_features(word)\n",
    "        sense = rule_based_disambiguate_word(word, dataset, features)\n",
    "        if sense is None:\n",
    "            X = vectorizer.transform([preprocessed_text])\n",
    "            predicted_senses = model.predict(X)\n",
    "\n",
    "            word_senses[word] = predicted_senses[0]\n",
    "            if word not in ambiguous_words:\n",
    "                ambiguous_words[word] = {'count': 0, 'meanings': []}\n",
    "            word_meanings = dataset.loc[(dataset['Word'] == word) & (dataset['Sense'].isin(predicted_senses)), 'Meaning'].unique()\n",
    "            if len(word_meanings) > 0:\n",
    "                ambiguous_words[word]['count'] += 1\n",
    "                ambiguous_words[word]['meanings'].extend(list(word_meanings))\n",
    "\n",
    "    # Remove words from ambiguous_words that have count = 0\n",
    "    ambiguous_words = {word: data for word, data in ambiguous_words.items() if data['count'] > 0}\n",
    "\n",
    "    # Remove word_senses for words not in the dataset\n",
    "    word_senses = {word: sense for word, sense in word_senses.items() if word in dataset['Word'].values}\n",
    "\n",
    "    return word_senses, ambiguous_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Interface Functions\n",
    "def disambiguate(output_text):\n",
    "    global input_text, dataset, model, vectorizer\n",
    "\n",
    "    text = input_text.get(\"1.0\", \"end\").strip()\n",
    "    if text:\n",
    "        word_senses, ambiguous_words = predict_sense(text, dataset, model, vectorizer)\n",
    "        ambiguous_words_count = 0\n",
    "        ambiguous_words_list = []\n",
    "\n",
    "        for word, data in ambiguous_words.items():\n",
    "            count = data['count']\n",
    "            word_meanings = data['meanings']\n",
    "\n",
    "            # Check if the word has any valid meanings in the dataset\n",
    "            valid_meanings = [meaning for meaning in word_meanings if meaning != 'Meaning not found']\n",
    "            if valid_meanings:\n",
    "                ambiguous_words_count += 1\n",
    "                ambiguous_words_list.append(word)\n",
    "\n",
    "        # Display ambiguous word senses count\n",
    "        output_text.insert(tk.END, f\"Ambiguous words count: {ambiguous_words_count}\\n\\n\")\n",
    "\n",
    "        # Display ambiguous word senses and their meanings\n",
    "        for word in ambiguous_words_list:\n",
    "            data = ambiguous_words[word]\n",
    "            count = data['count']\n",
    "            word_meanings = data['meanings']\n",
    "            output_text.insert(tk.END, f\"Word: {word}\\n\")\n",
    "            # output_text.insert(tk.END, f\"Senses count: {count}\\n\")\n",
    "            output_text.insert(tk.END, \"Meanings:\\n\")\n",
    "            output_text.insert(tk.END, \"\\n\".join([f\"{i+1}. {meaning}\" for i, meaning in enumerate(word_meanings)]))\n",
    "            output_text.insert(tk.END, \"\\n\\n\")\n",
    "\n",
    "        if ambiguous_words_count == 0:\n",
    "            output_text.insert(tk.END, \"No ambiguous words found\\n\\n\")\n",
    "\n",
    "        # Print disambiguated meanings\n",
    "        for word, sense in word_senses.items():\n",
    "            if word not in ambiguous_words_list:\n",
    "                meanings = dataset.loc[dataset['Word'] == word, 'Meaning'].values\n",
    "                output_text.insert(tk.END, f\"Word: {word}\\n\")\n",
    "                # output_text.insert(tk.END, f\"Senses count: {sense}\\n\")\n",
    "                output_text.insert(tk.END, \"Meanings:\\n\")\n",
    "                output_text.insert(tk.END, \"\\n\".join([f\"{i+1}. {meaning}\" for i, meaning in enumerate(meanings)]))\n",
    "                output_text.insert(tk.END, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.8676470588235294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91901\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\91901\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\91901\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "def measure_performance(dataset, model, vectorizer, output_text):\n",
    "    # Prepare the test data\n",
    "    test_data = dataset.sample(frac=0.2, random_state=42)  # Use 20% of the dataset for testing\n",
    "    test_X = vectorizer.transform(test_data['Preprocessed'])\n",
    "    test_y = test_data['Sense']\n",
    "\n",
    "    # Predict the senses\n",
    "    predicted_y = model.predict(test_X)\n",
    "\n",
    "    # Calculate and display the classification report\n",
    "    report = classification_report(test_y, predicted_y)\n",
    "    output_text.insert(tk.END, \"Classification Report:\\n\")\n",
    "    output_text.insert(tk.END, report)\n",
    "    output_text.insert(tk.END, \"\\n\")\n",
    "\n",
    "    # Calculate and display the accuracy\n",
    "    accuracy = accuracy_score(test_y, predicted_y)\n",
    "    output_text.insert(tk.END, f\"Accuracy: {accuracy}\\n\")\n",
    "\n",
    "def main():\n",
    "    global input_text, dataset, model, vectorizer, output_text\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset()\n",
    "\n",
    "    # Train the machine learning model\n",
    "    model, vectorizer = train_model(dataset)\n",
    "\n",
    "    # Measure the performance\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Word Sense Disambiguation\")\n",
    "    root.geometry(\"400x400\")\n",
    "\n",
    "    output_text = tk.Text(root, height=15)\n",
    "    output_text.pack()\n",
    "\n",
    "    measure_performance(dataset, model, vectorizer, output_text)\n",
    "\n",
    "    label = tk.Label(root, text=\"Enter a sentence:\")\n",
    "    label.pack()\n",
    "\n",
    "    input_text = tk.Text(root, height=5)\n",
    "    input_text.pack()\n",
    "\n",
    "    button = tk.Button(root, text=\"Disambiguate\", command=lambda: disambiguate(output_text))\n",
    "    button.pack()\n",
    "\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\91901\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\91901\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\91901\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\91901\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\91901\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\91901\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91901\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91901\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91901\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import nltk\n",
    " nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91901\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\91901\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\91901/nltk_data', 'C:\\\\Users\\\\91901\\\\anaconda3\\\\nltk_data', 'C:\\\\Users\\\\91901\\\\anaconda3\\\\share\\\\nltk_data', 'C:\\\\Users\\\\91901\\\\anaconda3\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\91901\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\91901\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
